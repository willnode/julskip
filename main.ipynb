{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from skimage.feature import local_binary_pattern\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math, copy, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['fitur_set', 'idx_cls', 'label_names', 'target_label']>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('dataset.h5')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"fitur_set\": shape (484, 224, 224, 3), type \"<f2\">"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.get('fitur_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "    \n",
    "class LBP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LBP, self).__init__()\n",
    "        self.radius = 1\n",
    "        self.n_points = 8 * self.radius\n",
    "        self.METHOD = 'default'\n",
    "        \n",
    "    def forward(self, data):\n",
    "        _, w, h, d = data.shape\n",
    "        lbp = lambda data: local_binary_pattern(data.reshape(w, h), self.n_points, self.radius, self.METHOD).reshape(w, h, 1)\n",
    "        lbps = [[lbp(a[:,:,i]) for i in range(d)] for a in data]\n",
    "        lbps = [np.concatenate(lbp, axis=2) / 255 for lbp in lbps]\n",
    "        return torch.from_numpy(np.array(lbps)).float()\n",
    "\n",
    "class Flatten2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten2D, self).__init__()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return torch.transpose(data.view(data.size(0), -1, data.size(3)), 1, 2)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return data.view(data.size(0), -1)\n",
    "\n",
    "class Argmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Argmax, self).__init__()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return torch.argmax(data, dim=1)\n",
    "\n",
    "class CNNLBP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLBP, self).__init__()\n",
    "        cnn1d = torch.nn.Sequential(\n",
    "            Flatten2D(),\n",
    "            nn.Conv1d(3, 12, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(12, 24, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(24 * 1024, 128),\n",
    "        )\n",
    "        self.cnnrgb, self.cnnhsi = clones(cnn1d, 2)\n",
    "        self.lbp = LBP()\n",
    "        self.softmax = torch.nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(128 * 2, 6),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        rgb = data\n",
    "        hsi = self.lbp(data)\n",
    "        rgb = self.cnnrgb(rgb)\n",
    "        hsi = self.cnnhsi(hsi)\n",
    "        return self.softmax(torch.cat((rgb, hsi), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1663, 0.1694, 0.1784, 0.1588, 0.1703, 0.1567],\n",
       "         [0.1648, 0.1699, 0.1783, 0.1580, 0.1721, 0.1570],\n",
       "         [0.1654, 0.1709, 0.1787, 0.1570, 0.1691, 0.1590],\n",
       "         [0.1640, 0.1691, 0.1768, 0.1601, 0.1711, 0.1588],\n",
       "         [0.1621, 0.1690, 0.1808, 0.1551, 0.1734, 0.1596],\n",
       "         [0.1651, 0.1726, 0.1750, 0.1580, 0.1711, 0.1582],\n",
       "         [0.1646, 0.1694, 0.1776, 0.1582, 0.1718, 0.1586],\n",
       "         [0.1631, 0.1710, 0.1826, 0.1561, 0.1713, 0.1558],\n",
       "         [0.1641, 0.1695, 0.1783, 0.1576, 0.1693, 0.1613],\n",
       "         [0.1681, 0.1685, 0.1770, 0.1572, 0.1730, 0.1562]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 32, 32, 3) / math.e / 2 + 0.5\n",
    "y = nn.functional.one_hot(torch.arange(0, 10) % 6).float()\n",
    "model = CNNLBP()\n",
    "y_pred = model(x)\n",
    "y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.17544097900390626\n",
      "Epoch: 10, Loss: 1.753701674938202\n",
      "Epoch: 20, Loss: 1.7510527491569519\n",
      "Epoch: 30, Loss: 1.7474077224731446\n",
      "Epoch: 40, Loss: 1.743290936946869\n",
      "Epoch: 50, Loss: 1.7388588190078735\n",
      "Epoch: 60, Loss: 1.7341361045837402\n",
      "Epoch: 70, Loss: 1.729101872444153\n",
      "Epoch: 80, Loss: 1.7237156748771667\n",
      "Epoch: 90, Loss: 1.7179197192192077\n",
      "Epoch: 100, Loss: 1.7116468667984008\n",
      "Epoch: 110, Loss: 1.7048240184783936\n",
      "Epoch: 120, Loss: 1.697361207008362\n",
      "Epoch: 130, Loss: 1.689153504371643\n",
      "Epoch: 140, Loss: 1.6800748348236083\n",
      "Epoch: 150, Loss: 1.6699762105941773\n",
      "Epoch: 160, Loss: 1.6586970448493958\n",
      "Epoch: 170, Loss: 1.646040952205658\n",
      "Epoch: 180, Loss: 1.6317957043647766\n",
      "Epoch: 190, Loss: 1.6157469630241394\n",
      "Epoch: 200, Loss: 1.5976421594619752\n",
      "Epoch: 210, Loss: 1.5772651433944702\n",
      "Epoch: 220, Loss: 1.5544533133506775\n",
      "Epoch: 230, Loss: 1.5291308641433716\n",
      "Epoch: 240, Loss: 1.5014620304107666\n",
      "Epoch: 250, Loss: 1.4719309091567994\n",
      "Epoch: 260, Loss: 1.4414178490638734\n",
      "Epoch: 270, Loss: 1.411102283000946\n",
      "Epoch: 280, Loss: 1.382322645187378\n",
      "Epoch: 290, Loss: 1.3562851071357727\n",
      "Epoch: 300, Loss: 1.3337639212608337\n",
      "Epoch: 310, Loss: 1.3149918079376222\n",
      "Epoch: 320, Loss: 1.2997521758079529\n",
      "Epoch: 330, Loss: 1.2875700116157531\n",
      "Epoch: 340, Loss: 1.277893078327179\n",
      "Epoch: 350, Loss: 1.2701984167098999\n",
      "Epoch: 360, Loss: 1.2640423059463501\n",
      "Epoch: 370, Loss: 1.2590681910514832\n",
      "Epoch: 380, Loss: 1.254999315738678\n",
      "Epoch: 390, Loss: 1.251626479625702\n",
      "Epoch: 400, Loss: 1.248793935775757\n",
      "Epoch: 410, Loss: 1.2463834285736084\n",
      "Epoch: 420, Loss: 1.2443042278289795\n",
      "Epoch: 430, Loss: 1.2424857139587402\n",
      "Epoch: 440, Loss: 1.2408724188804627\n",
      "Epoch: 450, Loss: 1.239419436454773\n",
      "Epoch: 460, Loss: 1.2380898237228393\n",
      "Epoch: 470, Loss: 1.2368517875671388\n",
      "Epoch: 480, Loss: 1.2356765389442443\n",
      "Epoch: 490, Loss: 1.234536921977997\n",
      "Epoch: 500, Loss: 1.2334048390388488\n",
      "Epoch: 510, Loss: 1.2322495460510254\n",
      "Epoch: 520, Loss: 1.2310349225997925\n",
      "Epoch: 530, Loss: 1.2297151923179626\n",
      "Epoch: 540, Loss: 1.228229033946991\n",
      "Epoch: 550, Loss: 1.2264899253845214\n",
      "Epoch: 560, Loss: 1.2243696689605712\n",
      "Epoch: 570, Loss: 1.2216711521148682\n",
      "Epoch: 580, Loss: 1.2180808544158936\n",
      "Epoch: 590, Loss: 1.213092529773712\n",
      "Epoch: 600, Loss: 1.2059061169624328\n",
      "Epoch: 610, Loss: 1.1954103589057923\n",
      "Epoch: 620, Loss: 1.1806951880455017\n",
      "Epoch: 630, Loss: 1.1626881122589112\n",
      "Epoch: 640, Loss: 1.1450344562530517\n",
      "Epoch: 650, Loss: 1.1305651426315309\n",
      "Epoch: 660, Loss: 1.1188796997070312\n",
      "Epoch: 670, Loss: 1.1089126229286195\n",
      "Epoch: 680, Loss: 1.1001624464988708\n",
      "Epoch: 690, Loss: 1.092522633075714\n",
      "Epoch: 700, Loss: 1.0860092759132385\n",
      "Epoch: 710, Loss: 1.0805463314056396\n",
      "Epoch: 720, Loss: 1.0759904026985168\n",
      "Epoch: 730, Loss: 1.0721932172775268\n",
      "Epoch: 740, Loss: 1.0690219044685363\n",
      "Epoch: 750, Loss: 1.0663609147071837\n",
      "Epoch: 760, Loss: 1.064114248752594\n",
      "Epoch: 770, Loss: 1.0622042655944823\n",
      "Epoch: 780, Loss: 1.0605687737464904\n",
      "Epoch: 790, Loss: 1.0591583490371703\n",
      "Epoch: 800, Loss: 1.0579337358474732\n",
      "Epoch: 810, Loss: 1.0568634390830993\n",
      "Epoch: 820, Loss: 1.0559222936630248\n",
      "Epoch: 830, Loss: 1.055089819431305\n",
      "Epoch: 840, Loss: 1.0543495178222657\n",
      "Epoch: 850, Loss: 1.0536878943443297\n",
      "Epoch: 860, Loss: 1.0530936598777771\n",
      "Epoch: 870, Loss: 1.0525578022003175\n",
      "Epoch: 880, Loss: 1.0520725131034852\n",
      "Epoch: 890, Loss: 1.0516313433647155\n",
      "Epoch: 900, Loss: 1.051228928565979\n",
      "Epoch: 910, Loss: 1.0508606076240539\n",
      "Epoch: 920, Loss: 1.0505223512649535\n",
      "Epoch: 930, Loss: 1.0502109408378602\n",
      "Epoch: 940, Loss: 1.049923312664032\n",
      "Epoch: 950, Loss: 1.0496570706367492\n",
      "Epoch: 960, Loss: 1.0494100213050843\n",
      "Epoch: 970, Loss: 1.049180281162262\n",
      "Epoch: 980, Loss: 1.0489660978317261\n",
      "Epoch: 990, Loss: 1.0487661004066466\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "running_loss, last_loss = 0., 0.\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {}, Loss: {}'.format(epoch, running_loss / 10))\n",
    "        running_loss = 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f86dcd9f989aa99c283cfd6c9364a61171dd56a31c823f2c3944837233534f92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
